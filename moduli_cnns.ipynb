{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d9e724",
   "metadata": {},
   "source": [
    "# Data loading & module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44166aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "# Here we will set up our data loading for testing\n",
    "train_transform = transforms.Compose([transforms.RandomRotation(10),\n",
    "                                     transforms.RandomHorizontalFlip(), \n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((.5, .5, .5), (.5, .5, .5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((.5, .5, .5), (.5, .5, .5))])\n",
    "\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=train_transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=test_transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "train_indeces = list(range(num_train))\n",
    "np.random.shuffle(train_indeces)\n",
    "split = int(np.floor(.1 * num_train))\n",
    "train_idx, valid_idx = train_indeces[split:], train_indeces[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "num_workers = 0\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=50,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=50, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=50, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f4064",
   "metadata": {},
   "source": [
    "# Defining distance functions for various manifolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The torus distance functions for an arbitrary dimensional torus. \n",
    "# When applied to tensors of the form a.shape() = [d, 1, n], b.shape() = [d, k, 1] it gives\n",
    "# the n x k matrix of distances between all pairs of points on a dimension d torus (S^1)^d\n",
    "# The parameter \"10\" here is an aribtrary size choice for the torus. Changing it (in both\n",
    "# terms) is equivalent to changing the coefficient of the regularizer. Changing them\n",
    "# independently is not an experiment we conducted\n",
    "def torus_distance(a, b):\n",
    "    return torch.min(torch.remainder(a - b, 10), torch.remainder(b-a, 10))\n",
    "\n",
    "\n",
    "# The Klein bottle distance is more complicated. The universal cover of a Klein bottle is \n",
    "# R^2, so we compute distance of two points p, p' on [0, 10] x [0, 10] with the Klein metric \n",
    "# as the minimum distance between p and Dp', where D ranges over the set of deck transform-\n",
    "# ations of R^2. For obvious reasons, we only need to compute the transforms on the 9 squares\n",
    "# centered on [0, 10] x [0, 10], which we do by applying the appropriate matrices.\n",
    "# Once again, the choice of 10 is arbitrary and could be changed\n",
    "embed_dim = 2\n",
    "\n",
    "M1 = torch.tensor([[1, 0, 10],\n",
    "                           [ 0, -1, 10],\n",
    "                           [ 0, 0, 1]], dtype=torch.float32)\n",
    "M2 = torch.tensor(np.array([[1, 0, -10],\n",
    "                           [ 0, -1, 10],\n",
    "                           [ 0, 0, 1]]), dtype=torch.float32)\n",
    "M3 = torch.tensor(np.array([[1, 0, 0],\n",
    "                           [ 0, 1, 10],\n",
    "                           [ 0, 0, 1]]), dtype=torch.float32)\n",
    "M4 = torch.tensor(np.array([[1, 0, 0],\n",
    "                           [ 0, 1, -10],\n",
    "                           [ 0, 0, 1]]), dtype=torch.float32)\n",
    "\n",
    "\n",
    "M1 = M1.to(device)\n",
    "M2 = M2.to(device)\n",
    "M3 = M3.to(device)\n",
    "M4 = M4.to(device)\n",
    "\n",
    "\n",
    "def klein_distance(a, b):\n",
    "    padder = nn.ConstantPad1d((0, 1), 1)\n",
    "    a = torch.remainder(a, 10)\n",
    "    b = torch.remainder(b, 10)\n",
    "    a = padder(a).transpose(0, 1)\n",
    "    b = padder(b).transpose(0, 1)\n",
    "    a = a.view(3, -1, 1)\n",
    "    #b = b.view(3, 1, -1)\n",
    "    return torch.min(torch.cat((torch.unsqueeze(torch.linalg.norm(a - b.view(3, 1, -1), dim = 0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(M1, b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(M2, b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(M3, b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(M4, b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(torch.matmul(M1, M3), b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(torch.matmul(M1, M4), b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(torch.matmul(M2, M3), b).view(3, 1, -1), dim=0), 0),\n",
    "                    torch.unsqueeze(torch.linalg.norm(a - torch.matmul(torch.matmul(M2, M4), b).view(3, 1, -1), dim=0), 0))), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The sphere distance is less complicated than the others. We don't need to work on the \n",
    "# universal covers, instead we just project onto a unit sphere and compute the angle between,\n",
    "# to determine the geodesic distance between the two points. \n",
    "# Technically there should be a factor of pi to accomodate that, but we absorb it into\n",
    "# our constant multiple and avoid the multiplication. /\n",
    "def sphere_distance(a, b):\n",
    "    return 10 * torch.nan_to_num(torch.acos(torch.inner(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebef8b",
   "metadata": {},
   "source": [
    "# Model implementation with learned embeddings\n",
    "Here is a sample model which learns optimal embeddings on the Klein bottle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCKlein(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCKlein, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.moduli_embed1 = nn.Parameter(torch.zeros((128*8*8, 2)).uniform_(0, 10))\n",
    "\n",
    "        self.moduli_embed2 = nn.Parameter(torch.zeros((256, 2)).uniform_(0, 10))\n",
    "        \n",
    "\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        \n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "                \n",
    "        self.fc1 = nn.Linear(128*8*8, 256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.5)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = F.relu(self.conv1(x))\n",
    "        max_out = self.maxpool1(conv_out)\n",
    "        conv_out = F.relu(self.conv2(max_out))\n",
    "        conv_out = self.maxpool2(conv_out)\n",
    "        conv_out = conv_out.view(-1, 128*8*8)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        fc1_out = F.relu(self.fc1(conv_out))\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        return fc2_out\n",
    "    \n",
    "        \n",
    "    \n",
    "    # These are the two regularizing terms we want to add to the loss. \n",
    "    # alpha should always be >= 0, and beta always <= 0\n",
    "    def topological_regularizer(self, alpha):\n",
    "        x = self.moduli_embed1#.view(2, -1, 1)\n",
    "        y = self.moduli_embed2#.view(2, 1, -1)\n",
    "        distance_matrix, _ = klein_distance(x, y)\n",
    "        return torch.mean((torch.mul(distance_matrix,torch.transpose(self.fc1.weight, 0, 1)))**alpha)\n",
    "    \n",
    "    def dispersement_regularizer(self):\n",
    "        x = self.moduli_embed1#.view(2, -1, 1)\n",
    "        y = self.moduli_embed2#.view(2, -1, 1)\n",
    "        distance_matrix, _ = klein_distance(x, x)\n",
    "        dist_2, _ = klein_distance(y, y)\n",
    "        zero_vec = torch.zeros(len(distance_matrix)).to(device)\n",
    "        zero_2 = torch.zeros(len(dist_2)).to(device)\n",
    "        distance_matrix = torch.log(distance_matrix+torch.tensor([.1]).to(device))\n",
    "        dist_2 = torch.log(dist_2+torch.tensor([.1]).to(device))\n",
    "        distance_matrix[range(len(distance_matrix)), range(len(distance_matrix))] = zero_vec\n",
    "        dist_2[range(len(dist_2)), range(len(dist_2))] = zero_2\n",
    "        return torch.mean(distance_matrix) + torch.mean(dist_2)\n",
    "    \n",
    "    def L2FC(self):\n",
    "        return torch.sum(self.fc1.weight**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "klein_test = FCKlein()\n",
    "klein_test = klein_test.to(device)\n",
    "\n",
    "\n",
    "# We tried using different optimizers and learning rates for the different parameters,\n",
    "# but ultimately found it had very little effect. \n",
    "my_list = ['moduli_embed1', 'moduli_embed2']\n",
    "moduli_params = [x[1] for x in list(filter(lambda kv: kv[0] in my_list, klein_test.named_parameters()))]\n",
    "base_params = [x[1] for x in list(filter(lambda kv: kv[0] not in my_list, klein_test.named_parameters()))]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.Adam([{'params': moduli_params, 'lr': 1e-3}], lr=1e-3)\n",
    "optimizer2 = optim.Adam([{'params': base_params}], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "topological_decay_lambda = .1\n",
    "topological_decay_alpha = 2\n",
    "moduli_dispersement_lambda = -.1\n",
    "L2factor = 0\n",
    "\n",
    "\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    test_CE = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Model training\n",
    "    klein_test.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        output = klein_test(data)\n",
    "        \n",
    "                \n",
    "        loss = criterion(output, target) + topological_decay_lambda * klein_test.topological_regularizer(topological_decay_alpha) + moduli_dispersement_lambda * klein_test.dispersement_regularizer() #+ L2factor*klein_test.L2FC()\n",
    "        \n",
    "        test_cross_entropy = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "                \n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        test_CE += test_cross_entropy.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    # Model eval\n",
    "    klein_test.eval()\n",
    "    num_correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = klein_test(data)\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        # Note I have left only the criterion loss here: this loss doesn't include the components from weight\n",
    "        # decay and moduli dispersement, only the cross entropy loss\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        num_correct += torch.sum(pred == target.data)\n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    test_CE = test_CE/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    accuracy = num_correct.item()/float(len(valid_loader.sampler))\n",
    "    \n",
    "    print('Epoch: {} \\tAccuracy: {:.6f} \\tTraining Cross Entropy: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, accuracy, test_CE, valid_loss))\n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(klein_test.state_dict(), 'model_klein_test.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c0704",
   "metadata": {},
   "source": [
    "# Model implementation with fixed random embeddings\n",
    "Very similar to the above, but now we no longer simultaneously train the manifold embedding. We use a higher dimensional torus, for variety. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim denotes what dimensional torus we work with. All of this could be\n",
    "# implemented inside the FCTorus __init__, if desired. We've left it outside\n",
    "# to emphasize the lack of modification during training. \n",
    "\n",
    "embed_dim = 9\n",
    "x = torch.zeros((128*8*8, embed_dim)).uniform_(0, 10).to(device)\n",
    "y = torch.zeros((256, embed_dim)).uniform_(0, 10).to(device)\n",
    "\n",
    "x = x.view(embed_dim, 1, -1)\n",
    "y = y.view(embed_dim, -1, 1)\n",
    "\n",
    "distance_matrix= torus_distance(x, y)\n",
    "distance_matrix = torch.linalg.norm(distance_matrix, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1179a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCTorus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCTorus, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        \n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "                \n",
    "        self.fc1 = nn.Linear(128*8*8, 256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.5)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = F.relu(self.conv1(x))\n",
    "        max_out = self.maxpool1(conv_out)\n",
    "        conv_out = F.relu(self.conv2(max_out))\n",
    "        conv_out = self.maxpool2(conv_out)\n",
    "        conv_out = conv_out.view(-1, 128*8*8)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        fc1_out = F.relu(self.fc1(conv_out))\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        return fc2_out\n",
    "\n",
    "    # These are the two regularizing terms we want to add to the loss. \n",
    "    # alpha should always be >= 0, and beta always <= 0\n",
    "    def topological_regularizer(self, alpha):\n",
    "        return torch.mean((torch.mul(distance_matrix, self.fc1.weight))**alpha)\n",
    "    \n",
    "    def dispersement_regularizer(self, beta):\n",
    "        x = self.moduli_embed1.weight.view(embed_dim, -1, 1)\n",
    "        y = self.moduli_embed2.weight.view(embed_dim, -1, 1)\n",
    "        distance_matrix = self.torus_distance(x, x.view(embed_dim, 1, -1))\n",
    "        dist_2 = self.torus_distance(y, y.view(embed_dim, 1, -1))\n",
    "        zero_vec = torch.zeros(len(distance_matrix)).cuda()\n",
    "        zero_2 = torch.zeros(len(dist_2)).cuda()\n",
    "        distance_matrix = (distance_matrix+torch.tensor([.01]).cuda())**beta\n",
    "        dist_2 = (dist_2+torch.tensor([.01]).cuda())**beta\n",
    "        distance_matrix[range(len(distance_matrix)), range(len(distance_matrix))] = zero_vec\n",
    "        dist_2[range(len(dist_2)), range(len(dist_2))] = zero_2\n",
    "        return torch.mean(distance_matrix) + torch.mean(dist_2)\n",
    "    \n",
    "    def L2FC(self):\n",
    "        return torch.sum(self.fc1.weight**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "torus_3_0 = FCTorus()\n",
    "\n",
    "torus_3_0 = torus_3_0.to(device)\n",
    "    \n",
    "my_list = ['moduli_embed1.weight', 'moduli_embed2.weight']\n",
    "moduli_params = [x[1] for x in list(filter(lambda kv: kv[0] in my_list, torus_3_0.named_parameters()))]\n",
    "base_params = [x[1] for x in list(filter(lambda kv: kv[0] not in my_list, torus_3_0.named_parameters()))]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.Adam([\n",
    "                            {'params': moduli_params, 'lr': 1e-3},\n",
    "                        ], lr=1e-3)\n",
    "optimizer2 = optim.Adam([{'params': base_params}], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "topological_decay_lambda = 1\n",
    "topological_decay_alpha = 2\n",
    "moduli_dispersement_lambda = .1\n",
    "moduli_dispersement_beta = -2\n",
    "L2factor = .001\n",
    "\n",
    "\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    test_CE = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Model training\n",
    "    torus_3_0.train()\n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        output = torus_3_0(data)\n",
    "        \n",
    "                \n",
    "        loss = criterion(output, target) + topological_decay_lambda * torus_3_0.topological_regularizer(topological_decay_alpha) #+ moduli_dispersement_lambda * torus_3_0.dispersement_regularizer(moduli_dispersement_beta)+L2factor*torus_3_0.L2FC()\n",
    "        \n",
    "        test_cross_entropy = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "                \n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        test_CE += test_cross_entropy.item()*data.size(0)\n",
    "        \n",
    "    \n",
    "    # Model eval\n",
    "    torus_3_0.eval()\n",
    "    num_correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = torus_3_0(data)\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        # Note I have left only the criterion loss here: this loss doesn't include the components from weight\n",
    "        # decay and moduli dispersement, only the cross entropy loss\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        num_correct += torch.sum(pred == target.data)\n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    test_CE = test_CE/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    accuracy = num_correct.item()/float(len(valid_loader.sampler))\n",
    "    \n",
    "    print('Epoch: {} \\tAccuracy: {:.6f} \\tTraining Cross Entropy: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, accuracy, test_CE, valid_loss))\n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(torus_3_0.state_dict(), 'model_hyperparameter_torus_3_0.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb536f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
